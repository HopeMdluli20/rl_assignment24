{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOSz5-AwzkVB",
        "outputId": "fde70df3-493c-4880-b102-40d64725058c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.10.1)\n",
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: grid2op in /usr/local/lib/python3.10/dist-packages (1.10.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grid2op) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from grid2op) (1.13.1)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from grid2op) (2.2.2)\n",
            "Requirement already satisfied: pandapower>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from grid2op) (2.14.11)\n",
            "Requirement already satisfied: tqdm>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from grid2op) (4.66.5)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from grid2op) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from grid2op) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from grid2op) (24.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from grid2op) (4.12.2)\n",
            "Requirement already satisfied: deepdiff in /usr/local/lib/python3.10/dist-packages (from pandapower>=2.2.2->grid2op) (8.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->grid2op) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->grid2op) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->grid2op) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->grid2op) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->grid2op) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->grid2op) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->grid2op) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.3->grid2op) (1.16.0)\n",
            "Requirement already satisfied: orderly-set==5.2.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff->pandapower>=2.2.2->grid2op) (5.2.2)\n",
            "Requirement already satisfied: lightsim2grid in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightsim2grid) (75.1.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from lightsim2grid) (24.1.2)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (from lightsim2grid) (2.13.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightsim2grid) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightsim2grid) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightsim2grid) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari,accept-rom-license]\"\n",
        "!pip install \"gymnasium[accept-rom-license\n",
        "!apt-get install -y swig\n",
        "!pip install grid2op\n",
        "!pip install lightsim2grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPROVEMENT 1: FEATURE SELECTION"
      ],
      "metadata": {
        "id": "jpsPJTJk2tKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import threading\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import grid2op\n",
        "from grid2op import gym_compat\n",
        "from grid2op.Parameters import Parameters\n",
        "from grid2op.Action import PlayableAction\n",
        "from grid2op.Observation import CompleteObservation\n",
        "from grid2op.Reward import L2RPNReward, N1Reward, CombinedScaledReward\n",
        "\n",
        "from lightsim2grid import LightSimBackend\n",
        "\n",
        "\n",
        "class PreprocessedEnv:\n",
        "    def __init__(self, env, selected_features=None):\n",
        "        self.env = env\n",
        "        self.selected_features = selected_features\n",
        "\n",
        "    def reset(self):\n",
        "        obs, info = self.env.reset()\n",
        "        return self.preprocess_observation(obs), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        return self.preprocess_observation(obs), reward, done, truncated, info\n",
        "\n",
        "    def preprocess_observation(self, observation):\n",
        "        # Convert the observation to a NumPy array for easier manipulation\n",
        "        observation_array = observation\n",
        "        selected_obs = np.array([observation_array[i] for i in self.selected_features])\n",
        "        # Normalize the selected features\n",
        "        normalized_obs = (selected_obs - selected_obs.mean()) / (selected_obs.std() + 1e-8)\n",
        "        return normalized_obs\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.env.seed(seed)\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        return self.env.render(*args, **kwargs)\n",
        "\n",
        "\n",
        "class Gym2OpEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self._backend = LightSimBackend()\n",
        "        self._env_name = \"l2rpn_case14_sandbox\"\n",
        "\n",
        "        action_class = PlayableAction\n",
        "        observation_class = CompleteObservation\n",
        "        reward_class = CombinedScaledReward\n",
        "\n",
        "        p = Parameters()\n",
        "        p.MAX_SUB_CHANGED = 4\n",
        "        p.MAX_LINE_STATUS_CHANGED = 4\n",
        "\n",
        "        self._g2op_env = grid2op.make(\n",
        "            self._env_name, backend=self._backend, test=False,\n",
        "            action_class=action_class, observation_class=observation_class,\n",
        "            reward_class=reward_class, param=p\n",
        "        )\n",
        "\n",
        "        cr = self._g2op_env.get_reward_instance()\n",
        "        cr.addReward(\"N1\", N1Reward(), 1.0)\n",
        "        cr.addReward(\"L2RPN\", L2RPNReward(), 1.0)\n",
        "        cr.initialize(self._g2op_env)\n",
        "\n",
        "        self.setup_observations()\n",
        "        self.setup_actions()\n",
        "\n",
        "    def setup_observations(self):\n",
        "        obs = self._g2op_env.reset()\n",
        "\n",
        "        self.obs_components = [\n",
        "            (\"rho\", obs.rho),\n",
        "            (\"line_status\", obs.line_status),\n",
        "            (\"actual_dispatch\", obs.actual_dispatch),\n",
        "            (\"load_p\", obs.load_p),\n",
        "            (\"load_q\", obs.load_q),\n",
        "            (\"topo_vect\", obs.topo_vect),\n",
        "            (\"time_before_cooldown_line\", obs.time_before_cooldown_line),\n",
        "            (\"time_before_cooldown_sub\", obs.time_before_cooldown_sub),\n",
        "        ]\n",
        "\n",
        "        total_size = sum(component[1].size for component in self.obs_components)\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(total_size,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        print(f\"Observation space defined with {total_size} features.\")\n",
        "\n",
        "    def _format_observation(self, obs):\n",
        "        return np.concatenate([obs.__dict__[comp[0]].flatten() for comp in self.obs_components]).astype(np.float32)\n",
        "\n",
        "    def setup_actions(self):\n",
        "        num_substations = self._g2op_env.n_sub\n",
        "        num_lines = self._g2op_env.n_line\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(num_lines + num_substations + 1)\n",
        "        print(f\"Action space defined with {self.action_space.n} discrete actions corresponding to lines, substations, and do nothing.\")\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        g2op_obs = self._g2op_env.reset(seed=seed)\n",
        "        obs = self._format_observation(g2op_obs)\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        g2op_action = self._convert_action(action)\n",
        "        g2op_obs, reward, done, info = self._g2op_env.step(g2op_action)\n",
        "        obs = self._format_observation(g2op_obs)\n",
        "        truncated = False\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def _convert_action(self, gym_action):\n",
        "        if gym_action == 0:  # Do nothing\n",
        "            return self._g2op_env.action_space()\n",
        "        else:\n",
        "            line_id = (gym_action - 1) // 2\n",
        "            new_status = 1 if (gym_action - 1) % 2 == 0 else -1\n",
        "            return self._g2op_env.action_space({\"set_line_status\": [(line_id, new_status)]})\n",
        "\n",
        "    def render(self):\n",
        "        return self._g2op_env.render()\n",
        "\n",
        "\n",
        "# A3C Network\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.actor = nn.Linear(256, num_actions)\n",
        "        self.critic = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        policy = F.softmax(self.actor(x), dim=-1)\n",
        "        value = self.critic(x)\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "# Worker for A3C\n",
        "class Worker(threading.Thread):\n",
        "    def __init__(self, global_model, optimizer, global_episode, global_step, max_steps, episode_rewards):\n",
        "        super(Worker, self).__init__()\n",
        "        base_env = Gym2OpEnv()\n",
        "        selected_features = [0, 5, 10]\n",
        "        self.env = PreprocessedEnv(base_env, selected_features)  # Use PreprocessedEnv to preprocess observations\n",
        "        self.local_model = ActorCritic(self.env.env.observation_space.shape[0], self.env.env.action_space.n)\n",
        "        self.global_model = global_model\n",
        "        self.optimizer = optimizer\n",
        "        self.global_episode = global_episode\n",
        "        self.global_step = global_step\n",
        "        self.max_steps = max_steps\n",
        "        self.episode_rewards = episode_rewards\n",
        "\n",
        "    def run(self):\n",
        "        while True:\n",
        "            with self.global_episode.get_lock():\n",
        "                if self.global_episode.value >= 100:  # Check if the limit is reached\n",
        "                    break\n",
        "                self.global_episode.value += 1\n",
        "                episode_num = self.global_episode.value\n",
        "\n",
        "            state, _ = self.env.reset()\n",
        "            self.local_model.load_state_dict(self.global_model.state_dict())\n",
        "\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            step = 0\n",
        "\n",
        "            while not done and step < self.max_steps:\n",
        "                policy, value = self.local_model(torch.FloatTensor(state))\n",
        "                action_dist = Categorical(policy)\n",
        "                action = action_dist.sample()\n",
        "\n",
        "                next_state, reward, done, truncated, info = self.env.step(action.item())\n",
        "                done = done or truncated\n",
        "\n",
        "                total_reward += reward\n",
        "                step += 1\n",
        "\n",
        "                self.update_global(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "\n",
        "            self.episode_rewards.append(total_reward)\n",
        "            print(f\"Episode: {episode_num}, Total Reward: {total_reward}, Steps: {step}\")\n",
        "\n",
        "    def update_global(self, state, action, reward, next_state, done):\n",
        "        self.optimizer.zero_grad()\n",
        "        policy, value = self.local_model(torch.FloatTensor(state))\n",
        "        _, next_value = self.local_model(torch.FloatTensor(next_state))\n",
        "        target = reward + 0.99 * next_value * (1 - int(done))\n",
        "        advantage = target - value\n",
        "        actor_loss = -torch.log(policy[action]) * advantage.detach()\n",
        "        critic_loss = advantage.pow(2)\n",
        "        loss = actor_loss + 0.5 * critic_loss\n",
        "        loss.backward()\n",
        "\n",
        "        for local_param, global_param in zip(self.local_model.parameters(), self.global_model.parameters()):\n",
        "            if global_param.grad is not None:\n",
        "                global_param.grad += local_param.grad\n",
        "            else:\n",
        "                global_param.grad = local_param.grad.clone()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        with self.global_step.get_lock():\n",
        "            self.global_step.value += 1\n",
        "\n",
        "\n",
        "def plot_rewards(episode_rewards):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(episode_rewards, label=\"Total Reward per Episode\", marker='o', linestyle='-')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('A3C Agent Performance: Total Rewards per Episode')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_a3c_performance(episode_rewards, smooth_factor=0.1):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot raw data\n",
        "    episodes = range(len(episode_rewards))\n",
        "    plt.plot(episodes, episode_rewards, alpha=0.3, color='blue', label='Raw')\n",
        "\n",
        "    # Plot smoothed data\n",
        "    smoothed_rewards = []\n",
        "    for i, reward in enumerate(episode_rewards):\n",
        "        if i == 0:\n",
        "            smoothed_rewards.append(reward)\n",
        "        else:\n",
        "            previous = smoothed_rewards[-1]\n",
        "            smoothed_rewards.append(previous * (1 - smooth_factor) + reward * smooth_factor)\n",
        "\n",
        "    plt.plot(episodes, smoothed_rewards, linewidth=2, color='blue', label='Smoothed')\n",
        "\n",
        "    plt.title('A3C Agent Performance: Total Rewards per Episode')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('a3c_performance.png')\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(env, model, num_episodes=100):\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        while not done and step < 100:\n",
        "            policy, _ = model(torch.FloatTensor(state))\n",
        "            action = torch.argmax(policy).item()\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            done = done or truncated\n",
        "            total_reward += reward\n",
        "            state = state\n",
        "            step += 1\n",
        "\n",
        "        print(f\"Evaluation Episode {episode + 1}: Total Reward = {total_reward}, Steps = {step}\")\n",
        "\n",
        "def main():\n",
        "    env = Gym2OpEnv()\n",
        "    global_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
        "    global_model.share_memory()\n",
        "\n",
        "    optimizer = optim.Adam(global_model.parameters(), lr=0.0002)\n",
        "    global_episode = multiprocessing.Value('i', 0)\n",
        "    global_step = multiprocessing.Value('i', 0)\n",
        "    episode_rewards = multiprocessing.Manager().list()\n",
        "\n",
        "    num_workers = max(1, multiprocessing.cpu_count() // 2)\n",
        "    workers = [Worker(global_model, optimizer, global_episode, global_step, max_steps=100, episode_rewards=episode_rewards) for _ in range(num_workers)]\n",
        "\n",
        "    [w.start() for w in workers]\n",
        "    [w.join() for w in workers]\n",
        "\n",
        "    evaluate_model(env, global_model)\n",
        "    # plot_rewards(episode_rewards)\n",
        "    # plot_a3c_performance(episode_rewards)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "SKU_aIhW2OhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPROVEMENT 2: REWARD SHAPING"
      ],
      "metadata": {
        "id": "CKjdXF4621hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import threading\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import grid2op\n",
        "from grid2op import gym_compat\n",
        "from grid2op.Parameters import Parameters\n",
        "from grid2op.Action import PlayableAction\n",
        "from grid2op.Observation import CompleteObservation\n",
        "from grid2op.Reward import L2RPNReward, N1Reward, CombinedScaledReward\n",
        "\n",
        "from lightsim2grid import LightSimBackend\n",
        "\n",
        "\n",
        "class PreprocessedEnv:\n",
        "    def __init__(self, env, selected_features=None):\n",
        "        self.env = env\n",
        "        self.selected_features = selected_features\n",
        "\n",
        "    def reset(self):\n",
        "        obs, info = self.env.reset()\n",
        "\n",
        "        return self.preprocess_observation(obs), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        reward = self.custom_reward_function(obs, reward, done, info)\n",
        "\n",
        "        return self.preprocess_observation(obs), reward, done, truncated, info\n",
        "\n",
        "    def preprocess_observation(self, observation):\n",
        "        observation_array = observation\n",
        "        selected_obs = np.array([observation_array[i] for i in self.selected_features])\n",
        "        normalized_obs = (selected_obs - selected_obs.mean()) / (selected_obs.std() + 1e-8)\n",
        "\n",
        "        return normalized_obs\n",
        "\n",
        "    def custom_reward_function(self, observation, reward, done, info):\n",
        "        custom_reward = reward\n",
        "        if 'voltage_violations' in info:\n",
        "            voltage_violations = info['voltage_violations']\n",
        "            custom_reward -= 10 * voltage_violations\n",
        "\n",
        "            load_shed = info.get('load_shed', 0)\n",
        "            custom_reward -= 5 * load_shed\n",
        "\n",
        "            line_trips = info.get('line_trips', 0)\n",
        "            custom_reward -= 2 * line_trips\n",
        "\n",
        "        return custom_reward\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.env.seed(seed)\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        return self.env.render(*args, **kwargs)\n",
        "\n",
        "\n",
        "class Gym2OpEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self._backend = LightSimBackend()\n",
        "        self._env_name = \"l2rpn_case14_sandbox\"\n",
        "\n",
        "        action_class = PlayableAction\n",
        "        observation_class = CompleteObservation\n",
        "        reward_class = CombinedScaledReward\n",
        "\n",
        "        p = Parameters()\n",
        "        p.MAX_SUB_CHANGED = 4\n",
        "        p.MAX_LINE_STATUS_CHANGED = 4\n",
        "\n",
        "        self._g2op_env = grid2op.make(\n",
        "            self._env_name, backend=self._backend, test=False,\n",
        "            action_class=action_class, observation_class=observation_class,\n",
        "            reward_class=reward_class, param=p\n",
        "        )\n",
        "\n",
        "        cr = self._g2op_env.get_reward_instance()\n",
        "        cr.addReward(\"N1\", N1Reward(), 1.0)\n",
        "        cr.addReward(\"L2RPN\", L2RPNReward(), 1.0)\n",
        "        cr.initialize(self._g2op_env)\n",
        "\n",
        "        self.setup_observations()\n",
        "        self.setup_actions()\n",
        "\n",
        "    def setup_observations(self):\n",
        "        obs = self._g2op_env.reset()\n",
        "\n",
        "        self.obs_components = [\n",
        "            (\"rho\", obs.rho),\n",
        "            (\"line_status\", obs.line_status),\n",
        "            (\"actual_dispatch\", obs.actual_dispatch),\n",
        "            (\"load_p\", obs.load_p),\n",
        "            (\"load_q\", obs.load_q),\n",
        "            (\"topo_vect\", obs.topo_vect),\n",
        "            (\"time_before_cooldown_line\", obs.time_before_cooldown_line),\n",
        "            (\"time_before_cooldown_sub\", obs.time_before_cooldown_sub),\n",
        "        ]\n",
        "\n",
        "        total_size = sum(component[1].size for component in self.obs_components)\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(total_size,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        print(f\"Observation space defined with {total_size} features.\")\n",
        "\n",
        "    def _format_observation(self, obs):\n",
        "        return np.concatenate([obs.__dict__[comp[0]].flatten() for comp in self.obs_components]).astype(np.float32)\n",
        "\n",
        "    def setup_actions(self):\n",
        "        num_substations = self._g2op_env.n_sub\n",
        "        num_lines = self._g2op_env.n_line\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(num_lines + num_substations + 1)\n",
        "        print(f\"Action space defined with {self.action_space.n} discrete actions corresponding to lines, substations, and do nothing.\")\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        g2op_obs = self._g2op_env.reset(seed=seed)\n",
        "        obs = self._format_observation(g2op_obs)\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        g2op_action = self._convert_action(action)\n",
        "        g2op_obs, reward, done, info = self._g2op_env.step(g2op_action)\n",
        "        obs = self._format_observation(g2op_obs)\n",
        "        truncated = False\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    def _convert_action(self, gym_action):\n",
        "        if gym_action == 0:  # Do nothing\n",
        "            return self._g2op_env.action_space()\n",
        "        else:\n",
        "            line_id = (gym_action - 1) // 2\n",
        "            new_status = 1 if (gym_action - 1) % 2 == 0 else -1\n",
        "            return self._g2op_env.action_space({\"set_line_status\": [(line_id, new_status)]})\n",
        "\n",
        "    def render(self):\n",
        "        return self._g2op_env.render()\n",
        "\n",
        "\n",
        "# A3C Network\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.actor = nn.Linear(256, num_actions)\n",
        "        self.critic = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        policy = F.softmax(self.actor(x), dim=-1)\n",
        "        value = self.critic(x)\n",
        "        return policy, value\n",
        "\n",
        "\n",
        "# Worker for A3C\n",
        "class Worker(threading.Thread):\n",
        "    def __init__(self, global_model, optimizer, global_episode, global_step, max_steps, episode_rewards):\n",
        "        super(Worker, self).__init__()\n",
        "        base_env = Gym2OpEnv()\n",
        "        selected_features = [0, 5, 10]\n",
        "        self.env = PreprocessedEnv(base_env, selected_features)\n",
        "        self.local_model = ActorCritic(self.env.env.observation_space.shape[0], self.env.env.action_space.n)\n",
        "        self.global_model = global_model\n",
        "        self.optimizer = optimizer\n",
        "        self.global_episode = global_episode\n",
        "        self.global_step = global_step\n",
        "        self.max_steps = max_steps\n",
        "        self.episode_rewards = episode_rewards\n",
        "\n",
        "    def run(self):\n",
        "        while True:\n",
        "            with self.global_episode.get_lock():\n",
        "                if self.global_episode.value >= 100:  # Check if the limit is reached\n",
        "                    break\n",
        "                self.global_episode.value += 1\n",
        "                episode_num = self.global_episode.value\n",
        "\n",
        "            state, _ = self.env.reset()\n",
        "            self.local_model.load_state_dict(self.global_model.state_dict())\n",
        "\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            step = 0\n",
        "\n",
        "            while not done and step < self.max_steps:\n",
        "                policy, value = self.local_model(torch.FloatTensor(state))\n",
        "                action_dist = Categorical(policy)\n",
        "                action = action_dist.sample()\n",
        "\n",
        "                next_state, reward, done, truncated, info = self.env.step(action.item())\n",
        "                done = done or truncated\n",
        "\n",
        "                total_reward += reward\n",
        "                step += 1\n",
        "\n",
        "                self.update_global(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "\n",
        "            self.episode_rewards.append(total_reward)\n",
        "            print(f\"Episode: {episode_num}, Total Reward: {total_reward}, Steps: {step}\")\n",
        "\n",
        "    def update_global(self, state, action, reward, next_state, done):\n",
        "        self.optimizer.zero_grad()\n",
        "        policy, value = self.local_model(torch.FloatTensor(state))\n",
        "        _, next_value = self.local_model(torch.FloatTensor(next_state))\n",
        "        target = reward + 0.99 * next_value * (1 - int(done))\n",
        "        advantage = target - value\n",
        "        actor_loss = -torch.log(policy[action]) * advantage.detach()\n",
        "        critic_loss = advantage.pow(2)\n",
        "        loss = actor_loss + 0.5 * critic_loss\n",
        "        loss.backward()\n",
        "\n",
        "        for local_param, global_param in zip(self.local_model.parameters(), self.global_model.parameters()):\n",
        "            if global_param.grad is not None:\n",
        "                global_param.grad += local_param.grad\n",
        "            else:\n",
        "                global_param.grad = local_param.grad.clone()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        with self.global_step.get_lock():\n",
        "            self.global_step.value += 1\n",
        "\n",
        "\n",
        "def plot_rewards(episode_rewards):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(episode_rewards, label=\"Total Reward per Episode\", marker='o', linestyle='-')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('A3C Agent Performance: Total Rewards per Episode')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_a3c_performance(episode_rewards, smooth_factor=0.1):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot raw data\n",
        "    episodes = range(len(episode_rewards))\n",
        "    plt.plot(episodes, episode_rewards, alpha=0.3, color='blue', label='Raw')\n",
        "\n",
        "    # Plot smoothed data\n",
        "    smoothed_rewards = []\n",
        "    for i, reward in enumerate(episode_rewards):\n",
        "        if i == 0:\n",
        "            smoothed_rewards.append(reward)\n",
        "        else:\n",
        "            previous = smoothed_rewards[-1]\n",
        "            smoothed_rewards.append(previous * (1 - smooth_factor) + reward * smooth_factor)\n",
        "\n",
        "    plt.plot(episodes, smoothed_rewards, linewidth=2, color='blue', label='Smoothed')\n",
        "\n",
        "    plt.title('A3C Agent Performance: Total Rewards per Episode')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('a3c_performance.png')\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(env, model, num_episodes=100):\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        while not done and step < 100:\n",
        "            policy, _ = model(torch.FloatTensor(state))\n",
        "            action = torch.argmax(policy).item()\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            done = done or truncated\n",
        "            total_reward += reward\n",
        "            state = state\n",
        "            step += 1\n",
        "\n",
        "        print(f\"Evaluation Episode {episode + 1}: Total Reward = {total_reward}, Steps = {step}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = Gym2OpEnv()\n",
        "    global_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
        "    global_model.share_memory()\n",
        "\n",
        "    optimizer = optim.Adam(global_model.parameters(), lr=0.0002)\n",
        "    global_episode = multiprocessing.Value('i', 0)\n",
        "    global_step = multiprocessing.Value('i', 0)\n",
        "    episode_rewards = multiprocessing.Manager().list()\n",
        "\n",
        "    num_workers = max(1, multiprocessing.cpu_count() // 2)\n",
        "    workers = [Worker(global_model, optimizer, global_episode, global_step, max_steps=100, episode_rewards=episode_rewards) for _ in range(num_workers)]\n",
        "\n",
        "    [w.start() for w in workers]\n",
        "    [w.join() for w in workers]\n",
        "\n",
        "    print(\"Debug2 - Episode Rewards Collected:\", list(episode_rewards))\n",
        "\n",
        "    evaluate_model(env, global_model)\n",
        "    # plot_rewards(episode_rewards)\n",
        "    # plot_a3c_performance(episode_rewards)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8PnppWIkzwKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844cef09-6ee8-4bc8-a66c-e85f5be9529c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/grid2op/MakeEnv/MakeFromPath.py:319: UserWarning: Impossible to load the coordinate of the substation with error: \"Grid2OpException EnvError \"Cannot find /root/data_grid2op/l2rpn_case14_sandbox/grid_layout.json. Dataset grid layout\"\". Expect some issue if you attempt to plot the grid.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/grid2op/MakeEnv/MakeFromPath.py:349: UserWarning: Dataset /root/data_grid2op/l2rpn_case14_sandbox/config.py doesn't have a valid graph layout. Expect some failures when attempting to plot the grid. Error was: [Errno 2] No such file or directory: '/root/data_grid2op/l2rpn_case14_sandbox/grid_layout.json'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:38: UserWarning: There were some Nan in the pp_net.trafo[\"tap_neutral\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_neutral\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:46: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_percent\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_percent\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:51: UserWarning: There were some Nan in the pp_net.trafo[\"tap_pos\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_pos\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:70: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_degree\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_degree\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_slack.py:56: UserWarning: LightSim will not consider the pandapower \"ext_grid\" as there are already generators tagged as slack bus\n",
            "  warnings.warn(\"LightSim will not consider the pandapower \\\"ext_grid\\\" as there \"\n",
            "/usr/local/lib/python3.10/dist-packages/grid2op/Environment/environment.py:297: UserWarning: No layout have been found for you grid (or the layout provided was corrupted). You will not be able to use the renderer, plot the grid etc. The error was \"File /root/data_grid2op/l2rpn_case14_sandbox/grid_layout.json does not exist\"\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space defined with 159 features.\n",
            "Action space defined with 35 discrete actions corresponding to lines, substations, and do nothing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/grid2op/MakeEnv/MakeFromPath.py:319: UserWarning: Impossible to load the coordinate of the substation with error: \"Grid2OpException EnvError \"Cannot find /root/data_grid2op/l2rpn_case14_sandbox/grid_layout.json. Dataset grid layout\"\". Expect some issue if you attempt to plot the grid.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/grid2op/MakeEnv/MakeFromPath.py:349: UserWarning: Dataset /root/data_grid2op/l2rpn_case14_sandbox/config.py doesn't have a valid graph layout. Expect some failures when attempting to plot the grid. Error was: [Errno 2] No such file or directory: '/root/data_grid2op/l2rpn_case14_sandbox/grid_layout.json'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:38: UserWarning: There were some Nan in the pp_net.trafo[\"tap_neutral\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_neutral\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:46: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_percent\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_percent\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:51: UserWarning: There were some Nan in the pp_net.trafo[\"tap_pos\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_pos\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_trafo.py:70: UserWarning: There were some Nan in the pp_net.trafo[\"tap_step_degree\"], they have been replaced by 0\n",
            "  warnings.warn(\"There were some Nan in the pp_net.trafo[\\\"tap_step_degree\\\"], they have been replaced by 0\")\n",
            "/usr/local/lib/python3.10/dist-packages/lightsim2grid/gridmodel/from_pandapower/_aux_add_slack.py:56: UserWarning: LightSim will not consider the pandapower \"ext_grid\" as there are already generators tagged as slack bus\n",
            "  warnings.warn(\"LightSim will not consider the pandapower \\\"ext_grid\\\" as there \"\n",
            "/usr/local/lib/python3.10/dist-packages/grid2op/Environment/environment.py:297: UserWarning: No layout have been found for you grid (or the layout provided was corrupted). You will not be able to use the renderer, plot the grid etc. The error was \"File /root/data_grid2op/l2rpn_case14_sandbox/grid_layout.json does not exist\"\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space defined with 159 features.\n",
            "Action space defined with 35 discrete actions corresponding to lines, substations, and do nothing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-31:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"<ipython-input-26-2c988161ca3b>\", line 202, in run\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-26-2c988161ca3b>\", line 164, in forward\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x3 and 159x128)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug2 - Episode Rewards Collected: []\n",
            "Evaluation Episode 1: Total Reward = 39.520487546920776, Steps = 100\n",
            "Evaluation Episode 2: Total Reward = 0.05397897958755493, Steps = 3\n",
            "Evaluation Episode 3: Total Reward = 35.46590131521225, Steps = 100\n",
            "Evaluation Episode 4: Total Reward = 37.539296209812164, Steps = 100\n",
            "Evaluation Episode 5: Total Reward = 37.48403298854828, Steps = 100\n",
            "Evaluation Episode 6: Total Reward = 37.20553860068321, Steps = 100\n",
            "Evaluation Episode 7: Total Reward = 38.18654680252075, Steps = 100\n",
            "Evaluation Episode 8: Total Reward = 37.55913692712784, Steps = 100\n",
            "Evaluation Episode 9: Total Reward = 35.950620740652084, Steps = 100\n",
            "Evaluation Episode 10: Total Reward = 34.77458503842354, Steps = 100\n",
            "Evaluation Episode 11: Total Reward = 33.21571362018585, Steps = 100\n",
            "Evaluation Episode 12: Total Reward = 36.79687216877937, Steps = 100\n",
            "Evaluation Episode 13: Total Reward = 35.63486009836197, Steps = 100\n",
            "Evaluation Episode 14: Total Reward = 35.25577396154404, Steps = 100\n",
            "Evaluation Episode 15: Total Reward = 35.24363449215889, Steps = 100\n",
            "Evaluation Episode 16: Total Reward = 37.682234823703766, Steps = 100\n",
            "Evaluation Episode 17: Total Reward = 37.66300955414772, Steps = 100\n",
            "Evaluation Episode 18: Total Reward = 35.02320346236229, Steps = 100\n",
            "Evaluation Episode 19: Total Reward = 35.66839116811752, Steps = 100\n",
            "Evaluation Episode 20: Total Reward = 34.34553322196007, Steps = 100\n",
            "Evaluation Episode 21: Total Reward = 36.58954069018364, Steps = 100\n",
            "Evaluation Episode 22: Total Reward = 36.73615336418152, Steps = 100\n",
            "Evaluation Episode 23: Total Reward = 34.066633850336075, Steps = 100\n",
            "Evaluation Episode 24: Total Reward = 35.88508468866348, Steps = 100\n",
            "Evaluation Episode 25: Total Reward = 36.17693057656288, Steps = 100\n",
            "Evaluation Episode 26: Total Reward = 39.07383945584297, Steps = 100\n",
            "Evaluation Episode 27: Total Reward = 35.036577463150024, Steps = 100\n",
            "Evaluation Episode 28: Total Reward = 37.51391679048538, Steps = 100\n",
            "Evaluation Episode 29: Total Reward = 38.36520180106163, Steps = 100\n",
            "Evaluation Episode 30: Total Reward = 36.70355451107025, Steps = 100\n",
            "Evaluation Episode 31: Total Reward = 39.19076880812645, Steps = 100\n",
            "Evaluation Episode 32: Total Reward = 36.451827704906464, Steps = 100\n",
            "Evaluation Episode 33: Total Reward = 36.615193992853165, Steps = 100\n",
            "Evaluation Episode 34: Total Reward = 37.86478394269943, Steps = 100\n",
            "Evaluation Episode 35: Total Reward = 35.7239648103714, Steps = 100\n",
            "Evaluation Episode 36: Total Reward = 36.59866350889206, Steps = 100\n",
            "Evaluation Episode 37: Total Reward = 36.852718234062195, Steps = 100\n",
            "Evaluation Episode 38: Total Reward = 0.0451754629611969, Steps = 3\n",
            "Evaluation Episode 39: Total Reward = 35.791465789079666, Steps = 100\n",
            "Evaluation Episode 40: Total Reward = 38.25949281454086, Steps = 100\n",
            "Evaluation Episode 41: Total Reward = 37.991385191679, Steps = 100\n",
            "Evaluation Episode 42: Total Reward = 34.13262954354286, Steps = 100\n",
            "Evaluation Episode 43: Total Reward = 37.0920048058033, Steps = 100\n",
            "Evaluation Episode 44: Total Reward = 38.006728410720825, Steps = 100\n",
            "Evaluation Episode 45: Total Reward = 36.95450672507286, Steps = 100\n",
            "Evaluation Episode 46: Total Reward = 35.871071964502335, Steps = 100\n",
            "Evaluation Episode 47: Total Reward = 37.6651351749897, Steps = 100\n",
            "Evaluation Episode 48: Total Reward = 36.27854299545288, Steps = 100\n",
            "Evaluation Episode 49: Total Reward = 38.83052736520767, Steps = 100\n",
            "Evaluation Episode 50: Total Reward = 38.449822545051575, Steps = 100\n",
            "Evaluation Episode 51: Total Reward = 36.058413207530975, Steps = 100\n",
            "Evaluation Episode 52: Total Reward = 38.65278711915016, Steps = 100\n",
            "Evaluation Episode 53: Total Reward = 36.24094843864441, Steps = 100\n",
            "Evaluation Episode 54: Total Reward = 38.348359018564224, Steps = 100\n",
            "Evaluation Episode 55: Total Reward = 35.03342619538307, Steps = 100\n",
            "Evaluation Episode 56: Total Reward = 38.03341221809387, Steps = 100\n",
            "Evaluation Episode 57: Total Reward = 37.67305454611778, Steps = 100\n",
            "Evaluation Episode 58: Total Reward = 37.10102841258049, Steps = 100\n",
            "Evaluation Episode 59: Total Reward = 36.50729990005493, Steps = 100\n",
            "Evaluation Episode 60: Total Reward = 35.66052985191345, Steps = 100\n",
            "Evaluation Episode 61: Total Reward = 35.01817390322685, Steps = 100\n",
            "Evaluation Episode 62: Total Reward = 37.876469284296036, Steps = 100\n",
            "Evaluation Episode 63: Total Reward = 37.35748887062073, Steps = 100\n",
            "Evaluation Episode 64: Total Reward = 36.678411066532135, Steps = 100\n",
            "Evaluation Episode 65: Total Reward = 35.809113532304764, Steps = 100\n",
            "Evaluation Episode 66: Total Reward = 38.711840987205505, Steps = 100\n",
            "Evaluation Episode 67: Total Reward = 35.842649042606354, Steps = 100\n",
            "Evaluation Episode 68: Total Reward = 35.71652129292488, Steps = 100\n",
            "Evaluation Episode 69: Total Reward = 37.469134122133255, Steps = 100\n",
            "Evaluation Episode 70: Total Reward = 34.116094559431076, Steps = 100\n",
            "Evaluation Episode 71: Total Reward = 34.81245070695877, Steps = 100\n",
            "Evaluation Episode 72: Total Reward = 34.61650165915489, Steps = 100\n",
            "Evaluation Episode 73: Total Reward = 36.48800367116928, Steps = 100\n",
            "Evaluation Episode 74: Total Reward = 34.95890638232231, Steps = 100\n",
            "Evaluation Episode 75: Total Reward = 36.26596197485924, Steps = 100\n",
            "Evaluation Episode 76: Total Reward = 40.11480310559273, Steps = 100\n",
            "Evaluation Episode 77: Total Reward = 37.6967776119709, Steps = 100\n",
            "Evaluation Episode 78: Total Reward = 35.295603185892105, Steps = 100\n",
            "Evaluation Episode 79: Total Reward = 35.71719631552696, Steps = 100\n",
            "Evaluation Episode 80: Total Reward = 35.88861119747162, Steps = 100\n",
            "Evaluation Episode 81: Total Reward = 36.07000803947449, Steps = 100\n",
            "Evaluation Episode 82: Total Reward = 35.73091459274292, Steps = 100\n",
            "Evaluation Episode 83: Total Reward = 35.90126916766167, Steps = 100\n",
            "Evaluation Episode 84: Total Reward = 34.02211806178093, Steps = 100\n",
            "Evaluation Episode 85: Total Reward = 39.026962995529175, Steps = 100\n",
            "Evaluation Episode 86: Total Reward = 35.36500772833824, Steps = 100\n",
            "Evaluation Episode 87: Total Reward = 33.20933818817139, Steps = 100\n",
            "Evaluation Episode 88: Total Reward = 36.76147612929344, Steps = 100\n",
            "Evaluation Episode 89: Total Reward = 38.01013660430908, Steps = 100\n",
            "Evaluation Episode 90: Total Reward = 38.66493582725525, Steps = 100\n",
            "Evaluation Episode 91: Total Reward = 35.301456809043884, Steps = 100\n",
            "Evaluation Episode 92: Total Reward = 36.1076934337616, Steps = 100\n",
            "Evaluation Episode 93: Total Reward = 36.87278300523758, Steps = 100\n",
            "Evaluation Episode 94: Total Reward = 37.263890594244, Steps = 100\n",
            "Evaluation Episode 95: Total Reward = 38.476391822099686, Steps = 100\n",
            "Evaluation Episode 96: Total Reward = 37.835309475660324, Steps = 100\n",
            "Evaluation Episode 97: Total Reward = 38.38524630665779, Steps = 100\n",
            "Evaluation Episode 98: Total Reward = 38.38687214255333, Steps = 100\n",
            "Evaluation Episode 99: Total Reward = 37.94874358177185, Steps = 100\n",
            "Evaluation Episode 100: Total Reward = 35.824388921260834, Steps = 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPBMP0eDFRXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}